# PROVIDER_AGENT or PROVIDER_TOOLS can be openai, watsonx, anthropic, azure, or any other backend litellm supports.
# MODEL_AGENT or MODEL_TOOLS can be a model name or a specific checkpoint like gpt-4o or gpt-4o-2024-11-20. For watsonx you need a prefix as well: ibm/granite-3-2-8b-instruct
# BASE_URL_AGENT or BASE_URL_TOOLS is provider dependent. For openai it can be left blank, for watsonx it is https://us-south.ml.cloud.ibm.com.
# API_VERSION_AGENT or API_VERSION_TOOLS is for azure. The correct api version should be indicated by the base_url.

### Agent ###
PROVIDER_AGENT=""
MODEL_AGENT=""
BASE_URL_AGENT=""
API_VERSION_AGENT=""
API_KEY_AGENT=""
REASONING_EFFORT_AGENT=""
SEED_AGENT=10
TOP_P_AGENT=0.95
TEMPERATURE_AGENT=0.0

### Tools ###
PROVIDER_TOOLS=""
MODEL_TOOLS=""
BASE_URL_TOOLS=""
API_VERSION_TOOLS=""
API_KEY_TOOLS=""
REASONING_EFFORT_TOOLS=""
SEED_TOOLS=10
TOP_P_TOOLS=0.95
TEMPERATURE_TOOLS=0.0
IS_NATIVE_FUNCTION_CALLING_SUPPORTED="True"

WX_PROJECT_ID=""
AGENT_TASK_DIRECTORY="config"
SRE_AGENT_EVALUATION_DIRECTORY=""
SRE_AGENT_NAME_VERSION_NUMBER=""
EXP_NAME=""
GOD_MODE="True"

KUBECONFIG=""
GRAFANA_URL="" 
GRAFANA_SERVICE_ACCOUNT_TOKEN="" 
TOPOLOGY_URL=""
STRUCTURED_UNSTRUCTURED_OUTPUT_DIRECTORY_PATH=""
